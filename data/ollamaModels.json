{
  "models": [
    {
      "name": "Llama 3.2 1B",
      "provider": "Meta",
      "parameters": 1,
      "category": "General",
      "minRAM": 4,
      "recommendedRAM": 8,
      "minVRAM": "N/A",
      "storageSize": "1.3GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Lightweight tasks, mobile devices, quick responses",
      "description": "Smallest Llama model, ideal for resource-constrained environments",
      "color": "#06b6d4",
      "features": ["Fast inference", "Low memory", "Mobile-ready"]
    },
    {
      "name": "Llama 3.2 3B",
      "provider": "Meta",
      "parameters": 3,
      "category": "General",
      "minRAM": 4,
      "recommendedRAM": 8,
      "minVRAM": "N/A",
      "storageSize": "2.0GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Chat, basic coding, text completion",
      "description": "Balanced small model for everyday tasks",
      "color": "#0ea5e9",
      "features": ["Edge deployment", "Fast", "Efficient"]
    },
    {
      "name": "Phi-3 Mini",
      "provider": "Microsoft",
      "parameters": 3.8,
      "category": "General",
      "minRAM": 4,
      "recommendedRAM": 8,
      "minVRAM": "N/A",
      "storageSize": "2.3GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Mobile apps, offline work, quick prototyping",
      "description": "Punches above its weight, great for constrained hardware",
      "color": "#8b5cf6",
      "features": ["Phone-capable", "Multilingual", "Efficient"]
    },
    {
      "name": "Gemma 2 2B",
      "provider": "Google",
      "parameters": 2,
      "category": "General",
      "minRAM": 4,
      "recommendedRAM": 8,
      "minVRAM": "N/A",
      "storageSize": "1.6GB",
      "contextWindow": 8,
      "quantization": "Q4_0",
      "useCase": "Edge devices, simple chat, text generation",
      "description": "Google's efficient small model with Flash Attention",
      "color": "#ec4899",
      "features": ["Flash Attention", "Memory efficient", "Fast"]
    },
    {
      "name": "Mistral 7B",
      "provider": "Mistral AI",
      "parameters": 7,
      "category": "General",
      "minRAM": 8,
      "recommendedRAM": 16,
      "minVRAM": "8GB (optional)",
      "storageSize": "4.1GB",
      "contextWindow": 32,
      "quantization": "Q4_0",
      "useCase": "General chat, writing, research, email drafting",
      "description": "Workhorse model - fast, accurate, and versatile",
      "color": "#f59e0b",
      "features": ["Fast inference", "Versatile", "Efficient"]
    },
    {
      "name": "Llama 3.1 8B",
      "provider": "Meta",
      "parameters": 8,
      "category": "General",
      "minRAM": 8,
      "recommendedRAM": 16,
      "minVRAM": "8GB (optional)",
      "storageSize": "4.7GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "General purpose, reasoning, creative writing",
      "description": "Superior performance in 8B class, optimized for local use",
      "color": "#0ea5e9",
      "features": ["128K context", "Strong reasoning", "Multilingual"]
    },
    {
      "name": "Gemma 2 9B",
      "provider": "Google",
      "parameters": 9,
      "category": "General",
      "minRAM": 8,
      "recommendedRAM": 16,
      "minVRAM": "8GB (optional)",
      "storageSize": "5.4GB",
      "contextWindow": 8,
      "quantization": "Q4_0",
      "useCase": "Conversational AI, content generation",
      "description": "High-performing model with efficiency optimizations",
      "color": "#ec4899",
      "features": ["Flash Attention", "Efficient", "Quality output"]
    },
    {
      "name": "Qwen3 8B",
      "provider": "Alibaba",
      "parameters": 8,
      "category": "General",
      "minRAM": 8,
      "recommendedRAM": 16,
      "minVRAM": "8GB (optional)",
      "storageSize": "4.7GB",
      "contextWindow": 32,
      "quantization": "Q4_0",
      "useCase": "Multilingual tasks, agentic workflows, coding",
      "description": "Latest Qwen with 119 language support and MoE architecture",
      "color": "#06b6d4",
      "features": ["119 languages", "40% faster", "Thinking mode"]
    },
    {
      "name": "CodeLlama 7B",
      "provider": "Meta",
      "parameters": 7,
      "category": "Coding",
      "minRAM": 8,
      "recommendedRAM": 16,
      "minVRAM": "8GB (optional)",
      "storageSize": "3.8GB",
      "contextWindow": 16,
      "quantization": "Q4_0",
      "useCase": "Code completion, debugging, simple scripts",
      "description": "Code-specialized Llama for 20+ programming languages",
      "color": "#10b981",
      "features": ["20+ languages", "Code completion", "Debugging"]
    },
    {
      "name": "DeepSeek Coder 6.7B",
      "provider": "DeepSeek",
      "parameters": 6.7,
      "category": "Coding",
      "minRAM": 8,
      "recommendedRAM": 16,
      "minVRAM": "8GB (optional)",
      "storageSize": "3.8GB",
      "contextWindow": 16,
      "quantization": "Q4_0",
      "useCase": "Code generation, cross-file changes, refactoring",
      "description": "Specialized coder trained on 2T tokens, 87 languages",
      "color": "#3b82f6",
      "features": ["87 languages", "2T tokens", "Cross-file edits"]
    },
    {
      "name": "CodeLlama 13B",
      "provider": "Meta",
      "parameters": 13,
      "category": "Coding",
      "minRAM": 16,
      "recommendedRAM": 32,
      "minVRAM": "16GB (optional)",
      "storageSize": "7.3GB",
      "contextWindow": 16,
      "quantization": "Q4_0",
      "useCase": "Complex debugging, architecture planning",
      "description": "Mid-size coding model for more complex tasks",
      "color": "#10b981",
      "features": ["Better reasoning", "Complex debugging", "Architecture"]
    },
    {
      "name": "Vicuna 13B",
      "provider": "LMSYS",
      "parameters": 13,
      "category": "General",
      "minRAM": 16,
      "recommendedRAM": 32,
      "minVRAM": "16GB (optional)",
      "storageSize": "7.3GB",
      "contextWindow": 2,
      "quantization": "Q4_0",
      "useCase": "Natural conversations, custom assistants, long chats",
      "description": "Fine-tuned for natural, non-robotic conversations",
      "color": "#8b5cf6",
      "features": ["Natural dialogue", "Long context", "Assistant-ready"]
    },
    {
      "name": "Qwen2.5 Coder 14B",
      "provider": "Alibaba",
      "parameters": 14,
      "category": "Coding",
      "minRAM": 16,
      "recommendedRAM": 32,
      "minVRAM": "16GB (optional)",
      "storageSize": "9.0GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Advanced coding, code reasoning, fixing",
      "description": "Latest code-specific Qwen with major improvements",
      "color": "#06b6d4",
      "features": ["128K context", "Code reasoning", "Multi-file"]
    },
    {
      "name": "DeepSeek-R1 14B",
      "provider": "DeepSeek",
      "parameters": 14,
      "category": "Reasoning",
      "minRAM": 16,
      "recommendedRAM": 32,
      "minVRAM": "16GB (optional)",
      "storageSize": "8.9GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Complex reasoning, math, scientific analysis",
      "description": "Reasoning model approaching O3 performance",
      "color": "#f59e0b",
      "features": ["Chain-of-thought", "Math reasoning", "Scientific"]
    },
    {
      "name": "CodeLlama 34B",
      "provider": "Meta",
      "parameters": 34,
      "category": "Coding",
      "minRAM": 32,
      "recommendedRAM": 64,
      "minVRAM": "24GB (recommended)",
      "storageSize": "19GB",
      "contextWindow": 16,
      "quantization": "Q4_0",
      "useCase": "Enterprise coding, complex debugging, architecture",
      "description": "Large coding model for professional development",
      "color": "#10b981",
      "features": ["Enterprise-grade", "Complex systems", "Best accuracy"]
    },
    {
      "name": "DeepSeek Coder 33B",
      "provider": "DeepSeek",
      "parameters": 33,
      "category": "Coding",
      "minRAM": 32,
      "recommendedRAM": 64,
      "minVRAM": "24GB (recommended)",
      "storageSize": "18GB",
      "contextWindow": 16,
      "quantization": "Q4_0",
      "useCase": "Professional coding, large codebases, refactoring",
      "description": "High-performance coder with extensive training",
      "color": "#3b82f6",
      "features": ["87 languages", "Large context", "Professional"]
    },
    {
      "name": "DeepSeek-R1 32B",
      "provider": "DeepSeek",
      "parameters": 32,
      "category": "Reasoning",
      "minRAM": 32,
      "recommendedRAM": 64,
      "minVRAM": "24GB (recommended)",
      "storageSize": "18GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Advanced reasoning, research, analysis",
      "description": "Powerful reasoning model for complex problems",
      "color": "#f59e0b",
      "features": ["Deep reasoning", "Math expert", "Research-grade"]
    },
    {
      "name": "Mixtral 8x7B",
      "provider": "Mistral AI",
      "parameters": 47,
      "category": "General",
      "minRAM": 32,
      "recommendedRAM": 64,
      "minVRAM": "24GB (recommended)",
      "storageSize": "26GB",
      "contextWindow": 32,
      "quantization": "Q4_0",
      "useCase": "Complex reasoning, multilingual, general tasks",
      "description": "Mixture of Experts model with efficient architecture",
      "color": "#f97316",
      "features": ["MoE architecture", "Multilingual", "Efficient"]
    },
    {
      "name": "Llama 3.1 70B",
      "provider": "Meta",
      "parameters": 70,
      "category": "General",
      "minRAM": 64,
      "recommendedRAM": 128,
      "minVRAM": "40GB+ (GPU required)",
      "storageSize": "40GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Deep analysis, research, enterprise applications",
      "description": "Flagship model for complex reasoning and analysis",
      "color": "#0ea5e9",
      "features": ["128K context", "Research-grade", "Multilingual"]
    },
    {
      "name": "DeepSeek-R1 70B",
      "provider": "DeepSeek",
      "parameters": 70,
      "category": "Reasoning",
      "minRAM": 64,
      "recommendedRAM": 128,
      "minVRAM": "40GB+ (GPU required)",
      "storageSize": "40GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Scientific research, advanced math, complex reasoning",
      "description": "State-of-the-art reasoning approaching O3 performance",
      "color": "#f59e0b",
      "features": ["97.3% HumanEval", "90.8% MMLU", "O3-level"]
    },
    {
      "name": "Mixtral 8x22B",
      "provider": "Mistral AI",
      "parameters": 141,
      "category": "General",
      "minRAM": 96,
      "recommendedRAM": 192,
      "minVRAM": "48GB+ (Multi-GPU)",
      "storageSize": "80GB",
      "contextWindow": 64,
      "quantization": "Q4_0",
      "useCase": "Enterprise reasoning, multilingual, complex tasks",
      "description": "Large MoE model with exceptional capabilities",
      "color": "#f97316",
      "features": ["MoE", "Multilingual", "Enterprise-scale"]
    },
    {
      "name": "Llama 3.3 70B",
      "provider": "Meta",
      "parameters": 70,
      "category": "General",
      "minRAM": 64,
      "recommendedRAM": 128,
      "minVRAM": "40GB+ (GPU required)",
      "storageSize": "40GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Latest general-purpose flagship model",
      "description": "Newest Llama iteration with improved capabilities",
      "color": "#0ea5e9",
      "features": ["Latest version", "Improved", "128K context"]
    },
    {
      "name": "Llama 3.1 405B",
      "provider": "Meta",
      "parameters": 405,
      "category": "General",
      "minRAM": 192,
      "recommendedRAM": 256,
      "minVRAM": "160GB+ (Multi-GPU required)",
      "storageSize": "231GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Cutting-edge research, enterprise AI, maximum capability",
      "description": "Largest open-source model, requires high-end hardware",
      "color": "#0ea5e9",
      "features": ["Largest open model", "Research-grade", "Enterprise"]
    },
    {
      "name": "DeepSeek-R1 671B",
      "provider": "DeepSeek",
      "parameters": 671,
      "category": "Reasoning",
      "minRAM": 192,
      "recommendedRAM": 256,
      "minVRAM": "320GB+ (8x A100 or H100)",
      "storageSize": "376GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Maximum reasoning capability, research frontiers",
      "description": "Largest reasoning model, approaching GPT-4 capability",
      "color": "#f59e0b",
      "features": ["O3-level", "Largest reasoning", "State-of-art"]
    },
    {
      "name": "LLaVA 7B",
      "provider": "LLaVA",
      "parameters": 7,
      "category": "Vision",
      "minRAM": 16,
      "recommendedRAM": 32,
      "minVRAM": "8GB (recommended)",
      "storageSize": "4.7GB",
      "contextWindow": 4,
      "quantization": "Q4_0",
      "useCase": "Image understanding, visual Q&A, multimodal tasks",
      "description": "Vision-language model for image and text understanding",
      "color": "#ec4899",
      "features": ["Vision encoder", "Multimodal", "Image Q&A"]
    },
    {
      "name": "LLaVA 13B",
      "provider": "LLaVA",
      "parameters": 13,
      "category": "Vision",
      "minRAM": 32,
      "recommendedRAM": 64,
      "minVRAM": "16GB (recommended)",
      "storageSize": "8GB",
      "contextWindow": 4,
      "quantization": "Q4_0",
      "useCase": "Advanced visual understanding, image analysis",
      "description": "Larger vision-language model with better accuracy",
      "color": "#ec4899",
      "features": ["Better vision", "Multimodal", "Detailed analysis"]
    },
    {
      "name": "Qwen2-VL 7B",
      "provider": "Alibaba",
      "parameters": 7,
      "category": "Vision",
      "minRAM": 16,
      "recommendedRAM": 32,
      "minVRAM": "8GB (recommended)",
      "storageSize": "4.5GB",
      "contextWindow": 32,
      "quantization": "Q4_0",
      "useCase": "Vision-language tasks, image understanding",
      "description": "Qwen's vision-language model with strong performance",
      "color": "#06b6d4",
      "features": ["Vision-language", "Multilingual", "High quality"]
    },
    {
      "name": "Qwen3-VL",
      "provider": "Alibaba",
      "parameters": 8,
      "category": "Vision",
      "minRAM": 16,
      "recommendedRAM": 32,
      "minVRAM": "8GB (recommended)",
      "storageSize": "5GB",
      "contextWindow": 32,
      "quantization": "Q4_0",
      "useCase": "Latest vision-language capabilities",
      "description": "Most powerful Qwen vision model to date",
      "color": "#06b6d4",
      "features": ["Flagship vision", "Latest", "Best quality"]
    },
    {
      "name": "Orca Mini 3B",
      "provider": "Microsoft",
      "parameters": 3,
      "category": "General",
      "minRAM": 4,
      "recommendedRAM": 8,
      "minVRAM": "N/A",
      "storageSize": "2.0GB",
      "contextWindow": 2,
      "quantization": "Q4_0",
      "useCase": "Old laptops, basic questions, lightweight tasks",
      "description": "Highly efficient model for constrained hardware",
      "color": "#8b5cf6",
      "features": ["Minimal resources", "Fast", "Efficient"]
    },
    {
      "name": "Dolphin 70B",
      "provider": "Cognitive Computations",
      "parameters": 70,
      "category": "Uncensored",
      "minRAM": 64,
      "recommendedRAM": 128,
      "minVRAM": "40GB+ (GPU required)",
      "storageSize": "40GB",
      "contextWindow": 8,
      "quantization": "Q4_0",
      "useCase": "Creative writing, brainstorming, open-ended exploration",
      "description": "Uncensored model for unrestricted creative work",
      "color": "#10b981",
      "features": ["Uncensored", "Creative", "Flexible"]
    },
    {
      "name": "WizardLM 13B",
      "provider": "WizardLM",
      "parameters": 13,
      "category": "Uncensored",
      "minRAM": 16,
      "recommendedRAM": 32,
      "minVRAM": "16GB (optional)",
      "storageSize": "7.3GB",
      "contextWindow": 2,
      "quantization": "Q4_0",
      "useCase": "Creative tasks, brainstorming, flexible exploration",
      "description": "Uncensored model for creative freedom",
      "color": "#8b5cf6",
      "features": ["Uncensored", "Creative", "Brainstorming"]
    },
    {
      "name": "Aya 8B",
      "provider": "Cohere",
      "parameters": 8,
      "category": "Multilingual",
      "minRAM": 8,
      "recommendedRAM": 16,
      "minVRAM": "8GB (optional)",
      "storageSize": "4.8GB",
      "contextWindow": 8,
      "quantization": "Q4_0",
      "useCase": "100+ languages, translation, cross-language work",
      "description": "Covers 100+ languages including less common ones",
      "color": "#ec4899",
      "features": ["100+ languages", "Translation", "Idioms"]
    },
    {
      "name": "Command-R 35B",
      "provider": "Cohere",
      "parameters": 35,
      "category": "Multilingual",
      "minRAM": 32,
      "recommendedRAM": 64,
      "minVRAM": "24GB (recommended)",
      "storageSize": "20GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Long multilingual documents, enterprise translation",
      "description": "Handles lengthy multilingual documents effectively",
      "color": "#ec4899",
      "features": ["Multilingual", "Long docs", "Enterprise"]
    },
    {
      "name": "GPT-OSS 1.5B",
      "provider": "OpenAI",
      "parameters": 1.5,
      "category": "Tools",
      "minRAM": 4,
      "recommendedRAM": 8,
      "minVRAM": "N/A",
      "storageSize": "1.1GB",
      "contextWindow": 128,
      "quantization": "Q4_0",
      "useCase": "Agentic tasks, tool use, reasoning",
      "description": "OpenAI's open-weight model for powerful reasoning",
      "color": "#10b981",
      "features": ["Tool use", "Agentic", "OpenAI"]
    }
  ]
}